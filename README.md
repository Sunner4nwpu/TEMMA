
This repository contains the code for TEMMA.

## Usage
The main.py gives an example of how to use the CNN-TE as well as the CNN-TEMMA models.

For the multi-modal CNN-TEMMA, please note that the input multi-stream features need to be concatenated first as in the example of the main.py


## Citing & Authors
if you find this repository helpful, please cite our publication:

```
@ARTICLE{9257201,
author={H. {Chen} and D. {Jiang} and H. {Sahli}},
journal={IEEE Transactions on Multimedia},
title={Transformer Encoder with Multi-modal Multi-head Attention for Continuous Affect Recognition},
year={2020},
doi={10.1109/TMM.2020.3037496}}
```

Contact person: Haifeng Chen,  Email: Sunner4nwpu@163.com


## Acknowledgement
* The codes of basemodel_1D is borrowed from [TCN](https://github.com/locuslab/TCN).
* The structure of the codebase is borrowed from [The Annotated Transformer](http://nlp.seas.harvard.edu/2018/04/03/attention.html)
